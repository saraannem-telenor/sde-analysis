{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Statkraft and Cerra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyproj import CRS, Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "shp_file = \"catchment_statkraft/catchment.shp\"\n",
    "catchment = gpd.read_file(shp_file)\n",
    "# Check the coordinate reference system (CRS)\n",
    "print(catchment.crs)\n",
    "# Plot the shapefile\n",
    "catchment.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the NetCDF file\n",
    "nc_file = \"atnasjo_snow_model.nc\"\n",
    "ds = xr.open_dataset(nc_file)\n",
    "# Print dataset metadata\n",
    "print(ds.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select time slice (e.g., first time step)\n",
    "ds.sel(time='2015-01-01')['sde'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and Assign CRS from NetCDF\n",
    "if 'crs_wkt' in ds['projection_lambert'].attrs:  # Adjust 'projection_lambert' if needed\n",
    "    netcdf_crs = CRS.from_wkt(ds['projection_lambert'].attrs['crs_wkt'])\n",
    "    print(\"Extracted CRS:\", netcdf_crs)\n",
    "else:\n",
    "    netcdf_crs = None\n",
    "    print(\"No CRS found in NetCDF file!\")\n",
    "    \n",
    "# Reproject Catchment Shapefile to NetCDF CRS\n",
    "if catchment.crs != netcdf_crs:\n",
    "    catchment = catchment.to_crs(netcdf_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data\n",
    "sde = ds['sde'].isel(time=0)  # First time step\n",
    "x = ds['x'].values  # X-coordinates (projected)\n",
    "y = ds['y'].values  # Y-coordinates (projected)\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ”¹ Fix Grid Cell Alignment by Creating an Edged Grid\n",
    "X, Y = np.meshgrid(\n",
    "    np.linspace(x.min(), x.max(), sde.shape[1] + 1),  # X should be 1 larger than sde.shape[1]\n",
    "    np.linspace(y.min(), y.max(), sde.shape[0] + 1)   # Y should be 1 larger than sde.shape[0]\n",
    ")\n",
    "\n",
    "# ðŸ”¹ Plot the Data Using pcolormesh()\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "c = ax.pcolormesh(X, Y, sde.values, cmap=\"Blues\", shading=\"flat\")  # Corrected!\n",
    "\n",
    "# Add a Colorbar\n",
    "cb = plt.colorbar(c, ax=ax)\n",
    "cb.set_label(\"Snow Depth (m)\")\n",
    "\n",
    "\n",
    "# Overlay Catchment Shapefile (Correctly Reprojected)\n",
    "catchment.boundary.plot(ax=ax, edgecolor=\"red\", linewidth=2, label=\"Catchment Boundary\")\n",
    "# Formatting\n",
    "ax.set_xlabel(\"X Coordinate (meters)\")\n",
    "ax.set_ylabel(\"Y Coordinate (meters)\")\n",
    "ax.set_title(\"Corrected Snow Depth Map with Catchment Overlay\")\n",
    "ax.set_aspect('equal')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the catchment with points, crs from .nc file converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract Data\n",
    "sde = ds['sde'].isel(time=0)  # First time step\n",
    "x = ds['x'].values  # X-coordinates (projected)\n",
    "y = ds['y'].values  # Y-coordinates (projected)\n",
    "\n",
    "# Create a meshgrid of X and Y\n",
    "X, Y = np.meshgrid(x, y)  # Convert 1D x and y arrays to a full grid\n",
    "\n",
    "# Flatten the arrays for creating GeoDataFrame\n",
    "X_flat = X.ravel()\n",
    "Y_flat = Y.ravel()\n",
    "\n",
    "# Create a GeoDataFrame of Grid Points\n",
    "grid_points = gpd.GeoDataFrame(\n",
    "    {'x': X_flat, 'y': Y_flat},\n",
    "    geometry=[Point(x, y) for x, y in zip(X_flat, Y_flat)],\n",
    "    crs=netcdf_crs\n",
    ")\n",
    "\n",
    "# Select Grid Points Inside the Catchment\n",
    "points_within = grid_points[grid_points.geometry.within(catchment.geometry.union_all())]\n",
    "\n",
    "# Print the points within the catchment\n",
    "print(\"Points within the catchment:\")\n",
    "print(points_within)\n",
    "\n",
    "# Plot the Data Using pcolormesh()\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Create an edged grid for pcolormesh\n",
    "X_edge, Y_edge = np.meshgrid(\n",
    "    np.linspace(x.min(), x.max(), sde.shape[1] + 1),  # X should be 1 larger than sde.shape[1]\n",
    "    np.linspace(y.min(), y.max(), sde.shape[0] + 1)   # Y should be 1 larger than sde.shape[0]\n",
    ")\n",
    "c = ax.pcolormesh(X_edge, Y_edge, sde.values, cmap=\"viridis\", shading=\"flat\")\n",
    "\n",
    "# Add a Colorbar\n",
    "cb = plt.colorbar(c, ax=ax)\n",
    "cb.set_label(\"Snow Depth (m)\")\n",
    "\n",
    "# Overlay Catchment Shapefile (Correctly Reprojected)\n",
    "catchment.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=2, label=\"Catchment Boundary\")\n",
    "\n",
    "# Plot the points within the catchment\n",
    "points_within.plot(ax=ax, \n",
    "                   color='red', \n",
    "                   markersize=20, \n",
    "                   label=\"Points within Catchment\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(\"X Coordinate (meters)\")\n",
    "ax.set_ylabel(\"Y Coordinate (meters)\")\n",
    "ax.set_title(\"Snow Depth Map with Catchment Overlay and Points within Catchment\")\n",
    "ax.set_aspect('equal')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sde = ds['sde'] \n",
    "time_series_data = []\n",
    "for _, row in points_within.iterrows():\n",
    "    x_pt, y_pt = row['x'], row['y']\n",
    "    idx_x = (np.abs(x - x_pt)).argmin()\n",
    "    idx_y = (np.abs(y - y_pt)).argmin()\n",
    "    sde_values = sde[:, idx_y, idx_x].values\n",
    "    for t, sde_value in zip(ds['time'].values, sde_values):\n",
    "        time_series_data.append({\n",
    "            'time': pd.Timestamp(t),\n",
    "            'x': x_pt,\n",
    "            'y': y_pt,\n",
    "            'statkraft_sde': sde_value\n",
    "        })\n",
    "# Create a DataFrame with the Time Series Data\n",
    "statkraft_sde_df = pd.DataFrame(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique (x, y) pairs\n",
    "unique_xy = statkraft_sde_df[['x', 'y']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Assign a unique index to each unique (x, y) pair\n",
    "unique_xy['unit'] = range(len(unique_xy))\n",
    "\n",
    "# Merge the unique indices back to the original DataFrame\n",
    "statkraft_sde_df = statkraft_sde_df.merge(unique_xy, on=['x', 'y'], how='left')\n",
    "statkraft_sde_df.set_index('time', inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Updated DataFrame with 'unit' column:\")\n",
    "print(statkraft_sde_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statkraft_sde_df.unit.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to have units as columns\n",
    "pivot_statkraft = statkraft_sde_df.pivot(columns='unit', \n",
    "                                         values='statkraft_sde')\n",
    "\n",
    "# Plot the time series for each unit with subplots\n",
    "pivot_statkraft.plot(subplots=True, \n",
    "                     sharex=True,\n",
    "                     #sharey=True, \n",
    "                     figsize=(15, 1 * len(pivot_statkraft.columns)), \n",
    "                     layout=(len(pivot_statkraft.columns), 1), \n",
    "                     legend=False)\n",
    "\n",
    "# Set the x-axis label for the last subplot\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer from netcdf_crs to EPSG:4326 (WGS84 lat/lon)\n",
    "transformer = Transformer.from_crs(netcdf_crs, \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "# Convert the x and y coordinates to longitude and latitude using .loc to avoid SettingWithCopyWarning\n",
    "points_within.loc[:, \"longitude\"], points_within.loc[:, \"latitude\"] = transformer.transform(\n",
    "    points_within[\"x\"].values, points_within[\"y\"].values\n",
    ")\n",
    "print(\"Converted UTM to Lat/Lon:\")\n",
    "print(points_within[['x', 'y', 'longitude', 'latitude']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run to retrieve CERRA data from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the Open-Meteo API client from openmeteo_requests\n",
    "\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Define API URL\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Initialize an empty list to store time-series results\n",
    "cerra_time_series_data = []\n",
    "\n",
    "# Define the start and end date for the time series (Match Statkraft!)\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2020-03-01\"\n",
    "\n",
    "# Loop through each grid point and get a full time series\n",
    "for index, row in points_within.iterrows():\n",
    "    params = {\n",
    "        \"latitude\": row[\"latitude\"],\n",
    "        \"longitude\": row[\"longitude\"],\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": \"snow_depth\",  # Fetching Hourly\n",
    "        \"cell_selection\": \"nearest\",\n",
    "        \"timezone\": \"UTC\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "\n",
    "        # Extract hourly snow depth\n",
    "        hourly = response.Hourly()\n",
    "        dates = pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        )\n",
    "        cerra_snow_depth = hourly.Variables(0).ValuesAsNumpy()\n",
    "\n",
    "        # Store results for the full time series\n",
    "        for i in range(len(dates)):\n",
    "            cerra_time_series_data.append({\n",
    "                \"x\": row[\"x\"],\n",
    "                \"y\": row[\"y\"],\n",
    "                \"longitude\": row[\"longitude\"],\n",
    "                \"latitude\": row[\"latitude\"],\n",
    "                \"date\": dates[i].date(),  # Convert to daily format\n",
    "                \"hourly_snow_depth\": cerra_snow_depth[i]\n",
    "            })\n",
    "\n",
    "        print(f\"Retrieved full time series for CERRA at ({row['latitude']:.4f}, {row['longitude']:.4f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {row['latitude']:.4f}, {row['longitude']:.4f}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "cerra_hourly_df = pd.DataFrame(cerra_time_series_data)\n",
    "\n",
    "# Convert hourly data to daily averages\n",
    "cerra_daily_df = cerra_hourly_df.groupby([\"x\", \"y\", \"longitude\", \"latitude\", \"date\"])[\"hourly_snow_depth\"].mean().reset_index()\n",
    "\n",
    "# Rename for clarity\n",
    "cerra_daily_df.rename(columns={\"hourly_snow_depth\": \"cerra_snow_depth\"}, inplace=True)\n",
    "\n",
    "# Display Retrieved Daily Time-Series Data\n",
    "print(\"Successfully Converted CERRA Data to Daily Resolution\")\n",
    "print(cerra_daily_df.head())\n",
    "\n",
    "cerra_daily_df.to_csv(\"cerra_snow_depth_new.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open CERRA data already fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cerra_sde_df = pd.read_csv(\"cerra_snow_depth_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'date' column to 'time' and convert it to datetime\n",
    "cerra_sde_df.rename(columns={'date': 'time'}, inplace=True)\n",
    "cerra_sde_df.rename(columns={'cerra_snow_depth': 'cerra_sde'}, inplace=True)\n",
    "cerra_sde_df['time'] = pd.to_datetime(cerra_sde_df['time'])\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Updated DataFrame with 'time' column as datetime index:\")\n",
    "print(cerra_sde_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the unique indices back to the original DataFrame\n",
    "cerra_sde_df = cerra_sde_df.merge(unique_xy, on=['x', 'y'], how='left')\n",
    "cerra_sde_df.set_index('time', inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Updated DataFrame with 'unit' column:\")\n",
    "print(cerra_sde_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to have units as columns\n",
    "pivot_cerra = cerra_sde_df.pivot_table(index='time', columns='unit', values='cerra_sde')\n",
    "\n",
    "\n",
    "# Plot the time series for each unit with subplots\n",
    "pivot_cerra.plot(subplots=True, \n",
    "                sharex=True, \n",
    "                sharey = True,\n",
    "                figsize=(15, 1 * len(pivot_cerra.columns)), \n",
    "                layout=(len(pivot_cerra.columns), 1), \n",
    "                legend=False)\n",
    "\n",
    "# Set the x-axis label for the last subplot\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series for each unit with subplots\n",
    "fig, axes = plt.subplots(nrows=len(pivot_cerra.columns), ncols=1, sharex=True, figsize=(15, 1 * len(pivot_cerra.columns)))\n",
    "\n",
    "for i, unit in enumerate(pivot_cerra.columns):\n",
    "    ax = axes[i]\n",
    "    pivot_cerra[unit].plot(ax=ax, label='CERRA', color='blue')\n",
    "    pivot_statkraft[unit].plot(ax=ax, label='Statkraft', color='orange')\n",
    "    \n",
    "    ax.set_title(f'Unit {unit}')\n",
    "    ax.legend()\n",
    "\n",
    "# Set the x-axis label for the last subplot\n",
    "axes[-1].set_xlabel('Time')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see whether the CERRA data is different wrt units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to have units as columns\n",
    "pivot_cerra = cerra_sde_df.pivot_table(index='time', columns='unit', values='cerra_sde')\n",
    "\n",
    "\n",
    "# Plot the time series for each unit with subplots\n",
    "pivot_cerra.plot(subplots=False, \n",
    "                sharex=True, \n",
    "                sharey = True,\n",
    "                figsize=(12, 3), \n",
    "                layout=(len(pivot_cerra.columns), 1), \n",
    "                legend=False)\n",
    "\n",
    "# Set the x-axis label for the last subplot\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statkraft_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
